{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca47aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763982941.418501  548915 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1763982941.422289  551248 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) HD Graphics 620 (KBL GT2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ditemukan 120 video. Sedang menganalisis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [00:00<?, ?it/s]W0000 00:00:1763982941.461112  551244 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "100%|██████████| 120/120 [00:50<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LAPORAN EDA VIDEO ===\n",
      "         Category     Subject         Video Name Resolution    FPS  \\\n",
      "0  non-drowsiness  subject_13     side_focus.mp4   1280x720  30.04   \n",
      "1  non-drowsiness  subject_13  front_talking.mp4  1920x1080  30.00   \n",
      "2  non-drowsiness  subject_13    front_focus.mp4  1920x1080  30.00   \n",
      "3  non-drowsiness  subject_13   side_talking.mp4   1280x720  30.04   \n",
      "4  non-drowsiness  subject_06     side_focus.mp4   1280x720  30.00   \n",
      "\n",
      "   Total Frames  Duration (s)  Brightness  Blur Score  Face Found  \n",
      "0           435         14.48      117.33      432.39        True  \n",
      "1           550         18.33      104.79      181.78        True  \n",
      "2           481         16.03      102.71      156.95        True  \n",
      "3           479         15.94      115.38      539.48        True  \n",
      "4           489         16.30      150.03      188.37        True  \n",
      "\n",
      "--- Ringkasan per Kategori ---\n",
      "                Duration (s)  Brightness  Blur Score\n",
      "Category                                            \n",
      "drowsiness            15.492   99.445333  327.049667\n",
      "non-drowsiness        17.147   97.257333  317.952000\n",
      "\n",
      "--- Cek Validitas Wajah ---\n",
      "Face Found\n",
      "True     115\n",
      "False      5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Laporan lengkap disimpan ke: video_eda_report.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- KONFIGURASI ---\n",
    "DATASET_ROOT = \"data/raw/train/\"  # Path folder dataset utama\n",
    "EXTENSIONS = ['*.mp4', '*.avi', '*.mov', '*.mkv'] # Format video yang dicari\n",
    "\n",
    "# Init MediaPipe untuk Cek Wajah (Mode Cepat)\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detector = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
    "\n",
    "def get_video_properties(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "\n",
    "    # 1. Properti Teknis\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps if fps > 0 else 0\n",
    "\n",
    "    # 2. Analisis Kualitas (Sampling Frame Tengah)\n",
    "    # Kita ambil frame di tengah video untuk menghindari black screen di awal\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    avg_brightness = 0\n",
    "    blur_score = 0\n",
    "    face_detected = False\n",
    "\n",
    "    if ret:\n",
    "        # Hitung Brightness (Rata-rata pixel grayscale)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        avg_brightness = np.mean(gray)\n",
    "\n",
    "        # Hitung Blur (Laplacian Variance)\n",
    "        # Semakin rendah angkanya, semakin blur\n",
    "        blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "        # Cek Keberadaan Wajah\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detector.process(frame_rgb)\n",
    "        if results.detections:\n",
    "            face_detected = True\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return {\n",
    "        \"Resolution\": f\"{width}x{height}\",\n",
    "        \"FPS\": round(fps, 2),\n",
    "        \"Total Frames\": total_frames,\n",
    "        \"Duration (s)\": round(duration, 2),\n",
    "        \"Brightness\": round(avg_brightness, 2),\n",
    "        \"Blur Score\": round(blur_score, 2),\n",
    "        \"Face Found\": face_detected\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Cari semua video secara rekursif\n",
    "    video_files = []\n",
    "    for ext in EXTENSIONS:\n",
    "        video_files.extend(glob.glob(os.path.join(DATASET_ROOT, \"**\", ext), recursive=True))\n",
    "    \n",
    "    print(f\"Ditemukan {len(video_files)} video. Sedang menganalisis...\")\n",
    "\n",
    "    data_report = []\n",
    "\n",
    "    for filepath in tqdm(video_files):\n",
    "        # Parse Path: data/raw/train/CATEGORY/SUBJECT/video.mp4\n",
    "        parts = filepath.split(os.sep)\n",
    "        category = parts[-3] # drowsiness / non-drowsiness\n",
    "        subject = parts[-2]  # subject_xx\n",
    "        filename = os.path.basename(filepath)\n",
    "\n",
    "        props = get_video_properties(filepath)\n",
    "        \n",
    "        if props:\n",
    "            row = {\n",
    "                \"Category\": category,\n",
    "                \"Subject\": subject,\n",
    "                \"Video Name\": filename,\n",
    "                **props\n",
    "            }\n",
    "            data_report.append(row)\n",
    "\n",
    "    # Buat DataFrame\n",
    "    df = pd.DataFrame(data_report)\n",
    "\n",
    "    # Tampilkan Statistik Ringkas\n",
    "    print(\"\\n=== LAPORAN EDA VIDEO ===\")\n",
    "    print(df.head())\n",
    "    print(\"\\n--- Ringkasan per Kategori ---\")\n",
    "    print(df.groupby(\"Category\")[[\"Duration (s)\", \"Brightness\", \"Blur Score\"]].mean())\n",
    "    print(\"\\n--- Cek Validitas Wajah ---\")\n",
    "    print(df[\"Face Found\"].value_counts())\n",
    "\n",
    "    # Simpan ke CSV\n",
    "    output_csv = \"video_eda_report.csv\"\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nLaporan lengkap disimpan ke: {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28232558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20b875b8",
   "metadata": {},
   "source": [
    "Ini adalah situasi \"Paling Rumit\" (Worst Case Scenario) dalam Computer Vision, tapi sangat umum terjadi.\n",
    "\n",
    "Mari kita bedah strateginya satu per satu. Kuncinya adalah: **Kita harus membuat Script Auto-Labeling kita \"Sadar Konteks\"**. Script tidak boleh buta-tuli melabeli semua titik yang dikasih MediaPipe.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Masalah Side View (Kamera Samping)\n",
    "\n",
    "Jika kamera dipasang di pilar A mobil (melihat profil wajah dari samping), atau sopir menoleh drastis:\n",
    "\n",
    "*   **Masalah:** Salah satu mata dan separuh mulut akan tersembunyi. MediaPipe terkadang \"halusinasi\" (menebak posisi mata yang hilang itu ada di balik kepala), tapi secara visual mata itu tidak ada.\n",
    "*   **Bahaya:** Jika kita melabeli mata yang \"tidak terlihat\" itu, Model YOLO akan bingung: *\"Disuruh cari mata, tapi di gambar cuma ada pipi/telinga. Aku harus belajar apa?\"*\n",
    "*   **Solusi di Script:** Gunakan **Head Pose Estimation (Yaw)**.\n",
    "    *   Hitung sudut toleh wajah.\n",
    "    *   Jika menoleh ke **Kiri** > 20 derajat $\\rightarrow$ **HANYA labeli Mata Kiri & Sudut Mulut Kiri**. Jangan labeli Mata Kanan.\n",
    "    *   Jika menoleh ke **Kanan** > 20 derajat $\\rightarrow$ **HANYA labeli Mata Kanan & Sudut Mulut Kanan**.\n",
    "    *   Label Wajah (`face`) tetap dibuat utuh.\n",
    "\n",
    "### 2. Masalah Kacamata Hitam (Sunglasses) & Kacamata Biasa\n",
    "\n",
    "*   **Kacamata Biasa (Bening):**\n",
    "    *   Biasanya aman. MediaPipe dan YOLO bisa melihat tembus pandang. Perlakuan: **Normal**.\n",
    "*   **Kacamata Hitam (Gelap):**\n",
    "    *   **Masalah:** Mata tidak terlihat. MediaPipe mungkin masih bisa tracking bentuk wajah, tapi akurasi kedipan (EAR) sangat rendah. YOLO tidak bisa melihat pola mata.\n",
    "    *   **Solusi di Script:** Deteksi area mata. Hitung **Rata-rata Kecerahan Pixel** di area mata.\n",
    "        *   Jika Pixel sangat gelap (Hitam) $\\rightarrow$ **JANGAN buat label mata (`o_eyes`/`c_eyes`)**.\n",
    "        *   Biarkan YOLO belajar bahwa di area itu ada Wajah, tapi tidak ada Mata yang bisa dideteksi.\n",
    "        *   *(Opsional: Labeli sebagai kelas baru `sunglasses`)*.\n",
    "\n",
    "### 3. Masalah Masker & Tangan (Covered Mouth)\n",
    "\n",
    "*   **Masalah:** Mulut tertutup.\n",
    "*   **Solusi di Script:**\n",
    "    *   Gunakan logika **MediaPipe Hands** (seperti diskusi sebelumnya) untuk tangan.\n",
    "    *   Gunakan logika **Pixel Variance/Color** atau sekadar asumsi dataset: Jika dataset `train/drowsiness` subject tertentu pakai masker terus, lebih baik **Folder Subject itu di-exclude** dari labeling mulut, ATAU kita labeli semua frame videonya sebagai `covered_mouth` (menggunakan logika deteksi warna solid di area mulut).\n",
    "\n",
    "### 4. Masalah Kombinasi (Combo Maut)\n",
    "\n",
    "Contoh: **Side View + Kacamata Hitam + Menguap Tutup Mulut.**\n",
    "*   Apa yang terlihat? Hanya telinga, pipi, kacamata hitam, dan punggung tangan.\n",
    "*   **Label apa yang harus dihasilkan?**\n",
    "    *   `face`: YES.\n",
    "    *   `o_eyes` / `c_eyes`: **NO** (Ketutup kacamata & angle).\n",
    "    *   `o_mouth` / `c_mouth`: **NO** (Ketutup tangan).\n",
    "    *   `covered_mouth`: **YES** (Deteksi tangan).\n",
    "*   **Kesimpulan:** Model YOLO hanya akan mendeteksi `face` dan `covered_mouth`.\n",
    "\n",
    "---\n",
    "\n",
    "### Rencana Update Kode: `prepare_data_advanced.py`\n",
    "\n",
    "Kita perlu meng-upgrade script kita menjadi sangat pintar. Script ini akan melakukan **Filtering Logika** sebelum menulis file `.txt`.\n",
    "\n",
    "Berikut adalah **Kode Lengkap** untuk menangani Side View, Kacamata Hitam (Pixel Check), dan Tangan (Covered Mouth).\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- KONFIGURASI ---\n",
    "ROOT_TRAIN_DIR = \"data/raw/train/\" \n",
    "OUTPUT_BASE_DIR = \"data/yolo_dataset_advanced/\"\n",
    "\n",
    "FRAME_INTERVAL = 10\n",
    "VAL_SUBJECTS = ['subject_13', 'subject_14', 'subject_15']\n",
    "\n",
    "# Thresholds\n",
    "EAR_THRESH = 0.25\n",
    "MAR_THRESH = 0.50\n",
    "DARKNESS_THRESH = 40  # Ambang batas kegelapan pixel (0-255) untuk deteksi kacamata hitam\n",
    "\n",
    "# Class ID\n",
    "CLS_FACE = 0\n",
    "CLS_O_EYES = 1\n",
    "CLS_C_EYES = 2\n",
    "CLS_O_MOUTH = 3\n",
    "CLS_C_MOUTH = 4\n",
    "CLS_COVERED_MOUTH = 5\n",
    "\n",
    "# Init MediaPipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands_detector = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "def calc_dist(p1, p2):\n",
    "    return np.linalg.norm(np.array([p1.x, p1.y]) - np.array([p2.x, p2.y]))\n",
    "\n",
    "def get_bbox_str(landmarks, h, w):\n",
    "    x_c = [lm.x for lm in landmarks]; y_c = [lm.y for lm in landmarks]\n",
    "    x_min, x_max = min(x_c), max(x_c)\n",
    "    y_min, y_max = min(y_c), max(y_c)\n",
    "    \n",
    "    # Padding\n",
    "    pad_x = (x_max - x_min) * 0.2\n",
    "    pad_y = (y_max - y_min) * 0.2\n",
    "    x_min = max(0, x_min - pad_x); y_min = max(0, y_min - pad_y)\n",
    "    x_max = min(1, x_max + pad_x); y_max = min(1, y_max + pad_y)\n",
    "\n",
    "    x_center = (x_min + x_max) / 2\n",
    "    y_center = (y_min + y_max) / 2\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    return f\"{x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
    "\n",
    "def is_region_dark(image, landmarks, h, w):\n",
    "    \"\"\"Cek apakah area landmarks (mata) gelap (kacamata hitam)\"\"\"\n",
    "    # Ambil bounding box area mata\n",
    "    x_c = [int(lm.x * w) for lm in landmarks]\n",
    "    y_c = [int(lm.y * h) for lm in landmarks]\n",
    "    x1, x2 = min(x_c), max(x_c)\n",
    "    y1, y2 = min(y_c), max(y_c)\n",
    "    \n",
    "    # Safety check image bounds\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = min(w, x2), min(h, y2)\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1: return False\n",
    "\n",
    "    roi = image[y1:y2, x1:x2]\n",
    "    gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    avg_brightness = np.mean(gray_roi)\n",
    "    \n",
    "    # Jika rata-rata brightness sangat rendah -> Hitam/Gelap\n",
    "    return avg_brightness < DARKNESS_THRESH\n",
    "\n",
    "def check_head_pose(landmarks):\n",
    "    \"\"\"\n",
    "    Return: 'frontal', 'left', 'right'\n",
    "    Logic: Bandingkan jarak hidung ke pipi kiri vs kanan\n",
    "    \"\"\"\n",
    "    nose = landmarks[1]\n",
    "    cheek_left = landmarks[234]\n",
    "    cheek_right = landmarks[454]\n",
    "    \n",
    "    dist_L = calc_dist(nose, cheek_left)\n",
    "    dist_R = calc_dist(nose, cheek_right)\n",
    "    \n",
    "    if dist_R == 0: return 'frontal'\n",
    "    ratio = dist_L / dist_R\n",
    "    \n",
    "    if ratio < 0.2: return 'left'   # Menoleh Kiri (Pipi kiri dekat hidung)\n",
    "    if ratio > 5.0: return 'right'  # Menoleh Kanan\n",
    "    return 'frontal'\n",
    "\n",
    "def is_hand_covering_mouth(face_lms, hand_results):\n",
    "    if not hand_results.multi_hand_landmarks: return False\n",
    "    mouth_idxs = [61, 291, 39, 181, 0, 17]\n",
    "    m_x = [face_lms[i].x for i in mouth_idxs]\n",
    "    m_y = [face_lms[i].y for i in mouth_idxs]\n",
    "    min_mx, max_mx = min(m_x), max(m_x)\n",
    "    min_my, max_my = min(m_y), max(m_y)\n",
    "    \n",
    "    for hand_lms in hand_results.multi_hand_landmarks:\n",
    "        for idx in [0, 5, 9, 13, 17, 8, 12, 16, 20]: # Cek semua titik penting tangan\n",
    "            hx, hy = hand_lms.landmark[idx].x, hand_lms.landmark[idx].y\n",
    "            if (min_mx - 0.05 < hx < max_mx + 0.05) and (min_my - 0.05 < hy < max_my + 0.05):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def process_dataset():\n",
    "    for split in ['train', 'val']:\n",
    "        os.makedirs(os.path.join(OUTPUT_BASE_DIR, \"images\", split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(OUTPUT_BASE_DIR, \"labels\", split), exist_ok=True)\n",
    "\n",
    "    video_files = glob.glob(os.path.join(ROOT_TRAIN_DIR, \"**\", \"*.*\"), recursive=True)\n",
    "    # Filter ekstensi video\n",
    "    video_files = [f for f in video_files if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
    "\n",
    "    print(f\"Total Video: {len(video_files)}\")\n",
    "\n",
    "    for video_path in tqdm(video_files):\n",
    "        # Parse Path\n",
    "        path_parts = video_path.split(os.sep)\n",
    "        # Sesuaikan index ini dengan struktur folder Anda!\n",
    "        # Misal: data/raw/train/drowsiness/subject_01/vid.mp4\n",
    "        try:\n",
    "            category = path_parts[-3] \n",
    "            subject = path_parts[-2]  \n",
    "        except:\n",
    "            category = \"unknown\"\n",
    "            subject = \"unknown\"\n",
    "            \n",
    "        filename = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        \n",
    "        split_type = \"val\" if subject in VAL_SUBJECTS else \"train\"\n",
    "        img_out_dir = os.path.join(OUTPUT_BASE_DIR, \"images\", split_type)\n",
    "        lbl_out_dir = os.path.join(OUTPUT_BASE_DIR, \"labels\", split_type)\n",
    "        file_prefix = f\"{split_type}_{category}_{subject}_{filename}\"\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            frame_count += 1\n",
    "            if frame_count % FRAME_INTERVAL != 0: continue \n",
    "            \n",
    "            h, w, _ = frame.shape\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # 1. Enhance Brightness (CLAHE) - Solusi Pencahayaan\n",
    "            lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "            l, a, b = cv2.split(lab)\n",
    "            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "            cl = clahe.apply(l)\n",
    "            limg = cv2.merge((cl,a,b))\n",
    "            frame_enhanced = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "            frame_rgb_enhanced = cv2.cvtColor(frame_enhanced, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Process MediaPipe on Enhanced Image\n",
    "            results_face = face_mesh.process(frame_rgb_enhanced)\n",
    "            results_hands = hands_detector.process(frame_rgb_enhanced)\n",
    "            \n",
    "            if not results_face.multi_face_landmarks: continue \n",
    "            \n",
    "            landmarks = results_face.multi_face_landmarks[0].landmark\n",
    "            labels = []\n",
    "            \n",
    "            # --- LOGIC UTAMA ---\n",
    "            \n",
    "            # 1. Face (Selalu ada)\n",
    "            labels.append(f\"{CLS_FACE} {get_bbox_str(landmarks, h, w)}\")\n",
    "            \n",
    "            # 2. Cek Head Pose (Side View)\n",
    "            pose = check_head_pose(landmarks)\n",
    "            \n",
    "            left_idxs = [33, 133, 160, 144, 158, 153]\n",
    "            right_idxs = [362, 263, 385, 380, 387, 373]\n",
    "            \n",
    "            # 3. Proses Mata (Dengan Cek Side View & Sunglasses)\n",
    "            \n",
    "            # Mata Kiri (Hanya proses jika pose frontal atau menoleh ke kiri/menampilkan sisi kiri)\n",
    "            if pose in ['frontal', 'left']:\n",
    "                # Cek Kacamata Hitam (Pixel check)\n",
    "                is_dark = is_region_dark(frame, [landmarks[i] for i in left_idxs], h, w)\n",
    "                if not is_dark:\n",
    "                    # Hitung EAR\n",
    "                    l_ear = (calc_dist(landmarks[160], landmarks[144]) + calc_dist(landmarks[158], landmarks[153])) / (2 * calc_dist(landmarks[33], landmarks[133]))\n",
    "                    cls = CLS_C_EYES if l_ear < EAR_THRESH else CLS_O_EYES\n",
    "                    labels.append(f\"{cls} {get_bbox_str([landmarks[i] for i in left_idxs], h, w)}\")\n",
    "            \n",
    "            # Mata Kanan (Hanya proses jika pose frontal atau menoleh ke kanan)\n",
    "            if pose in ['frontal', 'right']:\n",
    "                is_dark = is_region_dark(frame, [landmarks[i] for i in right_idxs], h, w)\n",
    "                if not is_dark:\n",
    "                    r_ear = (calc_dist(landmarks[385], landmarks[380]) + calc_dist(landmarks[387], landmarks[373])) / (2 * calc_dist(landmarks[362], landmarks[263]))\n",
    "                    cls = CLS_C_EYES if r_ear < EAR_THRESH else CLS_O_EYES\n",
    "                    labels.append(f\"{cls} {get_bbox_str([landmarks[i] for i in right_idxs], h, w)}\")\n",
    "\n",
    "            # 4. Proses Mulut (Dengan Cek Covered Mouth & Pose)\n",
    "            # Mulut biasanya terlihat dari kedua sisi kecuali toleh ekstrim\n",
    "            mouth_idxs = [61, 291, 39, 181, 0, 17]\n",
    "            \n",
    "            if is_hand_covering_mouth(landmarks, results_hands):\n",
    "                labels.append(f\"{CLS_COVERED_MOUTH} {get_bbox_str([landmarks[i] for i in mouth_idxs], h, w)}\")\n",
    "            else:\n",
    "                # Normal Mouth Logic\n",
    "                mouth_h = calc_dist(landmarks[13], landmarks[14])\n",
    "                mouth_w = calc_dist(landmarks[61], landmarks[291])\n",
    "                mar = mouth_h / mouth_w\n",
    "                cls = CLS_O_MOUTH if mar > MAR_THRESH else CLS_C_MOUTH\n",
    "                labels.append(f\"{cls} {get_bbox_str([landmarks[i] for i in mouth_idxs], h, w)}\")\n",
    "            \n",
    "            # --- Simpan ---\n",
    "            save_name = f\"{file_prefix}_fr{frame_count}\"\n",
    "            # Simpan frame ASLI (bukan enhanced) untuk training agar model belajar variasi cahaya\n",
    "            # ATAU simpan enhanced jika ingin model fokus ke struktur. \n",
    "            # Rekomendasi: Simpan ASLI agar model robust di malam hari.\n",
    "            cv2.imwrite(os.path.join(img_out_dir, save_name + \".jpg\"), frame)\n",
    "            \n",
    "            with open(os.path.join(lbl_out_dir, save_name + \".txt\"), \"w\") as f:\n",
    "                f.write(\"\\n\".join(labels))\n",
    "            \n",
    "        cap.release()\n",
    "\n",
    "    print(\"Selesai! Dataset Advanced Siap.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dataset()\n",
    "```\n",
    "\n",
    "### Penjelasan Fitur di Script Ini:\n",
    "\n",
    "1.  **CLAHE (`cv2.createCLAHE`)**:\n",
    "    *   Meningkatkan kecerahan/kontras gambar sebelum masuk MediaPipe. Ini membantu mendeteksi wajah di video gelap.\n",
    "    *   *Catatan:* Saya menyimpan gambar **ASLI** (`frame`) ke dataset, bukan gambar enhanced. Kenapa? Supaya YOLO belajar melihat dalam gelap. Enhanced image hanya dipakai untuk membantu MediaPipe membuat label (koordinat) yang akurat.\n",
    "\n",
    "2.  **`check_head_pose` (Side View Logic)**:\n",
    "    *   Jika wajah menoleh ke Kiri, script **TIDAK AKAN** membuat label untuk Mata Kanan (karena mungkin tersembunyi/distorsi).\n",
    "    *   Ini mencegah model belajar dari data sampah.\n",
    "\n",
    "3.  **`is_region_dark` (Sunglasses Logic)**:\n",
    "    *   Mengecek rata-rata warna pixel di dalam kotak mata. Jika terlalu gelap (< threshold), script **TIDAK AKAN** membuat label mata.\n",
    "    *   Jadi untuk orang berkacamata hitam, labelnya hanya `face` dan `mouth`.\n",
    "\n",
    "4.  **`is_hand_covering_mouth`**:\n",
    "    *   Melabeli `covered_mouth` jika tangan terdeteksi di depan bibir.\n",
    "\n",
    "Script ini adalah \"pabrik data\" yang cerdas. Jalankan ini, dan dataset YOLO Anda akan berkualitas sangat tinggi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365942ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- KONFIGURASI ---\n",
    "ROOT_TRAIN_DIR = \"data/raw/train/\" \n",
    "OUTPUT_BASE_DIR = \"data/yolo_dataset_advanced/\"\n",
    "\n",
    "FRAME_INTERVAL = 10\n",
    "VAL_SUBJECTS = ['subject_13', 'subject_14', 'subject_15']\n",
    "\n",
    "# Thresholds\n",
    "EAR_THRESH = 0.25\n",
    "MAR_THRESH = 0.50\n",
    "DARKNESS_THRESH = 40  # Ambang batas kegelapan pixel (0-255) untuk deteksi kacamata hitam\n",
    "\n",
    "# Class ID\n",
    "CLS_FACE = 0\n",
    "CLS_O_EYES = 1\n",
    "CLS_C_EYES = 2\n",
    "CLS_O_MOUTH = 3\n",
    "CLS_C_MOUTH = 4\n",
    "CLS_COVERED_MOUTH = 5\n",
    "\n",
    "# Init MediaPipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands_detector = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "def calc_dist(p1, p2):\n",
    "    return np.linalg.norm(np.array([p1.x, p1.y]) - np.array([p2.x, p2.y]))\n",
    "\n",
    "def get_bbox_str(landmarks, h, w):\n",
    "    x_c = [lm.x for lm in landmarks]; y_c = [lm.y for lm in landmarks]\n",
    "    x_min, x_max = min(x_c), max(x_c)\n",
    "    y_min, y_max = min(y_c), max(y_c)\n",
    "    \n",
    "    # Padding\n",
    "    pad_x = (x_max - x_min) * 0.2\n",
    "    pad_y = (y_max - y_min) * 0.2\n",
    "    x_min = max(0, x_min - pad_x); y_min = max(0, y_min - pad_y)\n",
    "    x_max = min(1, x_max + pad_x); y_max = min(1, y_max + pad_y)\n",
    "\n",
    "    x_center = (x_min + x_max) / 2\n",
    "    y_center = (y_min + y_max) / 2\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    return f\"{x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
    "\n",
    "def is_region_dark(image, landmarks, h, w):\n",
    "    \"\"\"Cek apakah area landmarks (mata) gelap (kacamata hitam)\"\"\"\n",
    "    # Ambil bounding box area mata\n",
    "    x_c = [int(lm.x * w) for lm in landmarks]\n",
    "    y_c = [int(lm.y * h) for lm in landmarks]\n",
    "    x1, x2 = min(x_c), max(x_c)\n",
    "    y1, y2 = min(y_c), max(y_c)\n",
    "    \n",
    "    # Safety check image bounds\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = min(w, x2), min(h, y2)\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1: return False\n",
    "\n",
    "    roi = image[y1:y2, x1:x2]\n",
    "    gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    avg_brightness = np.mean(gray_roi)\n",
    "    \n",
    "    # Jika rata-rata brightness sangat rendah -> Hitam/Gelap\n",
    "    return avg_brightness < DARKNESS_THRESH\n",
    "\n",
    "def check_head_pose(landmarks):\n",
    "    \"\"\"\n",
    "    Return: 'frontal', 'left', 'right'\n",
    "    Logic: Bandingkan jarak hidung ke pipi kiri vs kanan\n",
    "    \"\"\"\n",
    "    nose = landmarks[1]\n",
    "    cheek_left = landmarks[234]\n",
    "    cheek_right = landmarks[454]\n",
    "    \n",
    "    dist_L = calc_dist(nose, cheek_left)\n",
    "    dist_R = calc_dist(nose, cheek_right)\n",
    "    \n",
    "    if dist_R == 0: return 'frontal'\n",
    "    ratio = dist_L / dist_R\n",
    "    \n",
    "    if ratio < 0.2: return 'left'   # Menoleh Kiri (Pipi kiri dekat hidung)\n",
    "    if ratio > 5.0: return 'right'  # Menoleh Kanan\n",
    "    return 'frontal'\n",
    "\n",
    "def is_hand_covering_mouth(face_lms, hand_results):\n",
    "    if not hand_results.multi_hand_landmarks: return False\n",
    "    mouth_idxs = [61, 291, 39, 181, 0, 17]\n",
    "    m_x = [face_lms[i].x for i in mouth_idxs]\n",
    "    m_y = [face_lms[i].y for i in mouth_idxs]\n",
    "    min_mx, max_mx = min(m_x), max(m_x)\n",
    "    min_my, max_my = min(m_y), max(m_y)\n",
    "    \n",
    "    for hand_lms in hand_results.multi_hand_landmarks:\n",
    "        for idx in [0, 5, 9, 13, 17, 8, 12, 16, 20]: # Cek semua titik penting tangan\n",
    "            hx, hy = hand_lms.landmark[idx].x, hand_lms.landmark[idx].y\n",
    "            if (min_mx - 0.05 < hx < max_mx + 0.05) and (min_my - 0.05 < hy < max_my + 0.05):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def process_dataset():\n",
    "    for split in ['train', 'val']:\n",
    "        os.makedirs(os.path.join(OUTPUT_BASE_DIR, \"images\", split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(OUTPUT_BASE_DIR, \"labels\", split), exist_ok=True)\n",
    "\n",
    "    video_files = glob.glob(os.path.join(ROOT_TRAIN_DIR, \"**\", \"*.*\"), recursive=True)\n",
    "    # Filter ekstensi video\n",
    "    video_files = [f for f in video_files if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
    "\n",
    "    print(f\"Total Video: {len(video_files)}\")\n",
    "\n",
    "    for video_path in tqdm(video_files):\n",
    "        # Parse Path\n",
    "        path_parts = video_path.split(os.sep)\n",
    "        # Sesuaikan index ini dengan struktur folder Anda!\n",
    "        # Misal: data/raw/train/drowsiness/subject_01/vid.mp4\n",
    "        try:\n",
    "            category = path_parts[-3] \n",
    "            subject = path_parts[-2]  \n",
    "        except:\n",
    "            category = \"unknown\"\n",
    "            subject = \"unknown\"\n",
    "            \n",
    "        filename = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        \n",
    "        split_type = \"val\" if subject in VAL_SUBJECTS else \"train\"\n",
    "        img_out_dir = os.path.join(OUTPUT_BASE_DIR, \"images\", split_type)\n",
    "        lbl_out_dir = os.path.join(OUTPUT_BASE_DIR, \"labels\", split_type)\n",
    "        file_prefix = f\"{split_type}_{category}_{subject}_{filename}\"\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            frame_count += 1\n",
    "            if frame_count % FRAME_INTERVAL != 0: continue \n",
    "            \n",
    "            h, w, _ = frame.shape\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # 1. Enhance Brightness (CLAHE) - Solusi Pencahayaan\n",
    "            lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "            l, a, b = cv2.split(lab)\n",
    "            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "            cl = clahe.apply(l)\n",
    "            limg = cv2.merge((cl,a,b))\n",
    "            frame_enhanced = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "            frame_rgb_enhanced = cv2.cvtColor(frame_enhanced, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Process MediaPipe on Enhanced Image\n",
    "            results_face = face_mesh.process(frame_rgb_enhanced)\n",
    "            results_hands = hands_detector.process(frame_rgb_enhanced)\n",
    "            \n",
    "            if not results_face.multi_face_landmarks: continue \n",
    "            \n",
    "            landmarks = results_face.multi_face_landmarks[0].landmark\n",
    "            labels = []\n",
    "            \n",
    "            # --- LOGIC UTAMA ---\n",
    "            \n",
    "            # 1. Face (Selalu ada)\n",
    "            labels.append(f\"{CLS_FACE} {get_bbox_str(landmarks, h, w)}\")\n",
    "            \n",
    "            # 2. Cek Head Pose (Side View)\n",
    "            pose = check_head_pose(landmarks)\n",
    "            \n",
    "            left_idxs = [33, 133, 160, 144, 158, 153]\n",
    "            right_idxs = [362, 263, 385, 380, 387, 373]\n",
    "            \n",
    "            # 3. Proses Mata (Dengan Cek Side View & Sunglasses)\n",
    "            \n",
    "            # Mata Kiri (Hanya proses jika pose frontal atau menoleh ke kiri/menampilkan sisi kiri)\n",
    "            if pose in ['frontal', 'left']:\n",
    "                # Cek Kacamata Hitam (Pixel check)\n",
    "                is_dark = is_region_dark(frame, [landmarks[i] for i in left_idxs], h, w)\n",
    "                if not is_dark:\n",
    "                    # Hitung EAR\n",
    "                    l_ear = (calc_dist(landmarks[160], landmarks[144]) + calc_dist(landmarks[158], landmarks[153])) / (2 * calc_dist(landmarks[33], landmarks[133]))\n",
    "                    cls = CLS_C_EYES if l_ear < EAR_THRESH else CLS_O_EYES\n",
    "                    labels.append(f\"{cls} {get_bbox_str([landmarks[i] for i in left_idxs], h, w)}\")\n",
    "            \n",
    "            # Mata Kanan (Hanya proses jika pose frontal atau menoleh ke kanan)\n",
    "            if pose in ['frontal', 'right']:\n",
    "                is_dark = is_region_dark(frame, [landmarks[i] for i in right_idxs], h, w)\n",
    "                if not is_dark:\n",
    "                    r_ear = (calc_dist(landmarks[385], landmarks[380]) + calc_dist(landmarks[387], landmarks[373])) / (2 * calc_dist(landmarks[362], landmarks[263]))\n",
    "                    cls = CLS_C_EYES if r_ear < EAR_THRESH else CLS_O_EYES\n",
    "                    labels.append(f\"{cls} {get_bbox_str([landmarks[i] for i in right_idxs], h, w)}\")\n",
    "\n",
    "            # 4. Proses Mulut (Dengan Cek Covered Mouth & Pose)\n",
    "            # Mulut biasanya terlihat dari kedua sisi kecuali toleh ekstrim\n",
    "            mouth_idxs = [61, 291, 39, 181, 0, 17]\n",
    "            \n",
    "            if is_hand_covering_mouth(landmarks, results_hands):\n",
    "                labels.append(f\"{CLS_COVERED_MOUTH} {get_bbox_str([landmarks[i] for i in mouth_idxs], h, w)}\")\n",
    "            else:\n",
    "                # Normal Mouth Logic\n",
    "                mouth_h = calc_dist(landmarks[13], landmarks[14])\n",
    "                mouth_w = calc_dist(landmarks[61], landmarks[291])\n",
    "                mar = mouth_h / mouth_w\n",
    "                cls = CLS_O_MOUTH if mar > MAR_THRESH else CLS_C_MOUTH\n",
    "                labels.append(f\"{cls} {get_bbox_str([landmarks[i] for i in mouth_idxs], h, w)}\")\n",
    "            \n",
    "            # --- Simpan ---\n",
    "            save_name = f\"{file_prefix}_fr{frame_count}\"\n",
    "            # Simpan frame ASLI (bukan enhanced) untuk training agar model belajar variasi cahaya\n",
    "            # ATAU simpan enhanced jika ingin model fokus ke struktur. \n",
    "            # Rekomendasi: Simpan ASLI agar model robust di malam hari.\n",
    "            cv2.imwrite(os.path.join(img_out_dir, save_name + \".jpg\"), frame)\n",
    "            \n",
    "            with open(os.path.join(lbl_out_dir, save_name + \".txt\"), \"w\") as f:\n",
    "                f.write(\"\\n\".join(labels))\n",
    "            \n",
    "        cap.release()\n",
    "\n",
    "    print(\"Selesai! Dataset Advanced Siap.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f37fa8",
   "metadata": {},
   "source": [
    "### **KODE YANG SUDAH FINAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc28eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764033751.750046  548915 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1764033751.755308  573808 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) HD Graphics 620 (KBL GT2)\n",
      "I0000 00:00:1764033751.781885  548915 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "W0000 00:00:1764033751.790367  573802 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1764033751.791812  573815 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) HD Graphics 620 (KBL GT2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memproses 120 video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [00:00<?, ?it/s]W0000 00:00:1764033751.874840  573803 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764033751.901846  573810 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764033751.972518  573810 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "100%|██████████| 120/120 [31:22<00:00, 15.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROSES SELESAI: Dataset Final telah dibuat di 'data/yolo_dataset_master/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# KONFIGURASI UTAMA\n",
    "# ==========================================\n",
    "ROOT_TRAIN_DIR = \"data/raw/train/\" \n",
    "OUTPUT_BASE_DIR = \"data/processed\" \n",
    "\n",
    "FRAME_INTERVAL = 5 \n",
    "VAL_SUBJECTS = ['subject_13', 'subject_14', 'subject_15'] \n",
    "\n",
    "EAR_THRESH = 0.10\n",
    "MAR_THRESH = 0.50\n",
    "\n",
    "# --- KONFIGURASI POSE & OKLUSI ---\n",
    "SIDE_VIEW_RATIO_THRESH = 0.40 \n",
    "HEAD_DOWN_RATIO_THRESH = 0.35\n",
    "# Jika brightness mata < 50% dari brightness pipi -> Kacamata Hitam\n",
    "SUNGLASSES_RATIO_THRESH = 0.50\n",
    "MASK_HIST_DIFF_THRESH = 0.50    # Batas perbedaan warna Dahi vs Dagu (0=Sama, 1=Beda Jauh) \n",
    "\n",
    "# --- DEFINISI KELAS (7 KELAS) ---\n",
    "CLS_FACE = 0\n",
    "CLS_O_EYES = 1\n",
    "CLS_C_EYES = 2\n",
    "CLS_O_MOUTH = 3\n",
    "CLS_C_MOUTH = 4\n",
    "CLS_COVERED_MOUTH = 5\n",
    "CLS_SUNGLASSES = 6  \n",
    "\n",
    "# ==========================================\n",
    "# INISIALISASI MEDIAPIPE\n",
    "# ==========================================\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False, max_num_faces=1, \n",
    "    refine_landmarks=True, min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands_detector = mp_hands.Hands(\n",
    "    static_image_mode=False, max_num_hands=2,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "#             FUNGSI UTILITAS\n",
    "# ==========================================\n",
    "\n",
    "def apply_clahe(image):\n",
    "    \"\"\"Memperbaiki pencahayaan video\"\"\"\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    limg = cv2.merge((cl,a,b))\n",
    "    return cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def calc_dist(p1, p2):\n",
    "    return np.linalg.norm(np.array([p1.x, p1.y]) - np.array([p2.x, p2.y]))\n",
    "\n",
    "def get_bbox_str(landmarks, h, w):\n",
    "    \"\"\"Format Output YOLO: x_center y_center width height\"\"\"\n",
    "    x_c = [lm.x for lm in landmarks]\n",
    "    y_c = [lm.y for lm in landmarks]\n",
    "    x_min, x_max = min(x_c), max(x_c)\n",
    "    y_min, y_max = min(y_c), max(y_c)\n",
    "    \n",
    "    pad_x = (x_max - x_min) * 0.2\n",
    "    pad_y = (y_max - y_min) * 0.2\n",
    "    x_min = max(0, x_min - pad_x)\n",
    "    y_min = max(0, y_min - pad_y)\n",
    "    x_max = min(1, x_max + pad_x)\n",
    "    y_max = min(1, y_max + pad_y)\n",
    "\n",
    "    x_center = (x_min + x_max) / 2\n",
    "    y_center = (y_min + y_max) / 2\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    \n",
    "    return f\"{x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
    "\n",
    "def get_roi_brightness(image, landmarks, indices, h, w):\n",
    "    \"\"\"Menghitung rata-rata kecerahan (V channel HSV) pada area landmark tertentu\"\"\"\n",
    "    try:\n",
    "        x_c = [int(landmarks[i].x * w) for i in indices]\n",
    "        y_c = [int(landmarks[i].y * h) for i in indices]\n",
    "        x_min, x_max = min(x_c), max(x_c)\n",
    "        y_min, y_max = min(y_c), max(y_c)\n",
    "        \n",
    "        # Crop area\n",
    "        roi = image[y_min:y_max, x_min:x_max]\n",
    "        if roi.size == 0: return 0\n",
    "        \n",
    "        # Convert ke HSV dan ambil V (Brightness)\n",
    "        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "        brightness = np.mean(hsv[:, :, 2])\n",
    "        return brightness\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def is_wearing_sunglasses(image, landmarks, h, w):\n",
    "    \"\"\"Cek Kacamata Hitam (Pixel Brightness Ratio)\"\"\"\n",
    "    try:\n",
    "        # Area Mata Kiri & Pipi Kiri\n",
    "        eye_idx = [33, 133, 159, 145]\n",
    "        cheek_idx = [234, 93, 132, 58]\n",
    "        \n",
    "        def get_val(indices):\n",
    "            x_c = [int(landmarks[i].x * w) for i in indices]\n",
    "            y_c = [int(landmarks[i].y * h) for i in indices]\n",
    "            roi = image[min(y_c):max(y_c), min(x_c):max(x_c)]\n",
    "            if roi.size == 0: return 0\n",
    "            return np.mean(cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)[:,:,2])\n",
    "\n",
    "        eye_v = get_val(eye_idx)\n",
    "        cheek_v = get_val(cheek_idx)\n",
    "        \n",
    "        if cheek_v > 50 and (eye_v / cheek_v) < SUNGLASSES_RATIO_THRESH:\n",
    "            return True\n",
    "        return False\n",
    "    except: return False\n",
    "\n",
    "def analyze_head_pose(landmarks):\n",
    "    \"\"\"Cek apakah Menunduk (Head Down) atau Menoleh (Side View)\"\"\"\n",
    "    nose = landmarks[1]\n",
    "    left_cheek = landmarks[234]\n",
    "    right_cheek = landmarks[454]\n",
    "    chin = landmarks[152]\n",
    "    forehead = landmarks[10]\n",
    "\n",
    "    # 1. Cek Menunduk\n",
    "    face_height = calc_dist(forehead, chin)\n",
    "    nose_to_chin = calc_dist(nose, chin)\n",
    "    is_head_down = False\n",
    "    if face_height > 0:\n",
    "        if (nose_to_chin / face_height) < HEAD_DOWN_RATIO_THRESH:\n",
    "            is_head_down = True\n",
    "\n",
    "    # 2. Cek Menoleh (Yaw)\n",
    "    dist_left = calc_dist(nose, left_cheek)\n",
    "    dist_right = calc_dist(nose, right_cheek)\n",
    "    side_status = 'frontal'\n",
    "    \n",
    "    if dist_right > 0 and dist_left > 0:\n",
    "        if dist_left < dist_right * SIDE_VIEW_RATIO_THRESH:\n",
    "            side_status = 'look_left'  # Pipi kiri dekat kamera\n",
    "        elif dist_right < dist_left * SIDE_VIEW_RATIO_THRESH:\n",
    "            side_status = 'look_right' # Pipi kanan dekat kamera\n",
    "\n",
    "    return is_head_down, side_status\n",
    "\n",
    "def is_mouth_occluded(image, landmarks, hand_results, h, w):\n",
    "    \"\"\"\n",
    "    Cek apakah mulut tertutup:\n",
    "    1. Oleh Tangan (Geometry Check)\n",
    "    2. Oleh Masker (Color Histogram Check)\n",
    "    \"\"\"\n",
    "    mouth_idxs = [61, 291, 39, 181, 0, 17]\n",
    "    \n",
    "    # --- CEK 1: TANGAN ---\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        m_x = [landmarks[i].x for i in mouth_idxs]\n",
    "        m_y = [landmarks[i].y for i in mouth_idxs]\n",
    "        min_mx, max_mx = min(m_x), max(m_x)\n",
    "        min_my, max_my = min(m_y), max(m_y)\n",
    "        tol = 0.05\n",
    "        \n",
    "        for hand_lms in hand_results.multi_hand_landmarks:\n",
    "            # Ujung jari & buku jari\n",
    "            for idx in [0, 5, 9, 13, 17, 8, 12, 16, 20]:\n",
    "                hx, hy = hand_lms.landmark[idx].x, hand_lms.landmark[idx].y\n",
    "                if (min_mx-tol < hx < max_mx+tol) and (min_my-tol < hy < max_my+tol):\n",
    "                    return True # Kena Tangan\n",
    "\n",
    "    # --- CEK 2: MASKER (Warna) ---\n",
    "    try:\n",
    "        # Dahi vs Dagu\n",
    "        forehead_idx = [10, 338, 297, 67]\n",
    "        chin_idx = [164, 18, 200, 152]\n",
    "        \n",
    "        def get_roi(indices):\n",
    "            x_c = [int(landmarks[i].x * w) for i in indices]\n",
    "            y_c = [int(landmarks[i].y * h) for i in indices]\n",
    "            return image[min(y_c):max(y_c), min(x_c):max(x_c)]\n",
    "\n",
    "        f_roi = get_roi(forehead_idx)\n",
    "        c_roi = get_roi(chin_idx)\n",
    "        \n",
    "        if f_roi.size > 0 and c_roi.size > 0:\n",
    "            f_hsv = cv2.cvtColor(f_roi, cv2.COLOR_BGR2HSV)\n",
    "            c_hsv = cv2.cvtColor(c_roi, cv2.COLOR_BGR2HSV)\n",
    "            \n",
    "            # Bandingkan Hue & Saturation\n",
    "            hist_f = cv2.calcHist([f_hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
    "            hist_c = cv2.calcHist([c_hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
    "            cv2.normalize(hist_f, hist_f, 0, 1, cv2.NORM_MINMAX)\n",
    "            cv2.normalize(hist_c, hist_c, 0, 1, cv2.NORM_MINMAX)\n",
    "            \n",
    "            diff = cv2.compareHist(hist_f, hist_c, cv2.HISTCMP_BHATTACHARYYA)\n",
    "            if diff > MASK_HIST_DIFF_THRESH:\n",
    "                return True # Beda warna jauh (Masker)\n",
    "    except: pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "def save_data(img_dir, lbl_dir, prefix, count, img, labels):\n",
    "    save_name = f\"{prefix}_fr{count}\"\n",
    "    cv2.imwrite(os.path.join(img_dir, save_name + \".jpg\"), img)\n",
    "    with open(os.path.join(lbl_dir, save_name + \".txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(labels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# MAIN PROCESS\n",
    "# ==========================================\n",
    "def process_dataset():\n",
    "    # Buat struktur folder\n",
    "    for split in ['train', 'val']:\n",
    "        os.makedirs(os.path.join(OUTPUT_BASE_DIR, \"images\", split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(OUTPUT_BASE_DIR, \"labels\", split), exist_ok=True)\n",
    "\n",
    "    # Cari video\n",
    "    video_files = glob.glob(os.path.join(ROOT_TRAIN_DIR, \"**\", \"*.*\"), recursive=True)\n",
    "    valid_exts = ['.mp4', '.avi', '.mov', '.mkv']\n",
    "    video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in valid_exts]\n",
    "\n",
    "    print(f\"Memproses {len(video_files)} video...\")\n",
    "\n",
    "    for video_path in tqdm(video_files):\n",
    "        path_parts = video_path.split(os.sep)\n",
    "        category = path_parts[-3] \n",
    "        subject = path_parts[-2]  \n",
    "        filename = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        \n",
    "        # Tentukan Split\n",
    "        split_type = \"val\" if subject in VAL_SUBJECTS else \"train\"\n",
    "        img_out_dir = os.path.join(OUTPUT_BASE_DIR, \"images\", split_type)\n",
    "        lbl_out_dir = os.path.join(OUTPUT_BASE_DIR, \"labels\", split_type)\n",
    "        file_prefix = f\"{split_type}_{category}_{subject}_{filename}\"\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            frame_count += 1\n",
    "            if frame_count % FRAME_INTERVAL != 0: continue \n",
    "            \n",
    "            h, w, _ = frame.shape\n",
    "            \n",
    "            # 1. Preprocessing\n",
    "            frame_enhanced = apply_clahe(frame)\n",
    "            frame_rgb = cv2.cvtColor(frame_enhanced, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # 2. Inference MediaPipe\n",
    "            results_face = face_mesh.process(frame_rgb)\n",
    "            results_hands = hands_detector.process(frame_rgb)\n",
    "            \n",
    "            if not results_face.multi_face_landmarks: continue \n",
    "            landmarks = results_face.multi_face_landmarks[0].landmark\n",
    "            \n",
    "            labels = []\n",
    "            \n",
    "            # 3. Label WAJAH (Selalu ada)\n",
    "            labels.append(f\"{CLS_FACE} {get_bbox_str(landmarks, h, w)}\")\n",
    "            \n",
    "            # 4. Cek Pose Kepala\n",
    "            is_head_down, side_status = analyze_head_pose(landmarks)\n",
    "            \n",
    "            if is_head_down:\n",
    "                # Jika menunduk parah, jangan labeli mata/mulut (data tidak valid)\n",
    "                save_data(img_out_dir, lbl_out_dir, file_prefix, frame_count, frame_enhanced, labels)\n",
    "                continue\n",
    "\n",
    "            # 5. Label MATA (Cek Kacamata & Side View)\n",
    "            has_sunglasses = is_wearing_sunglasses(frame_enhanced, landmarks, h, w)\n",
    "            left_idxs = [33, 133, 160, 144, 158, 153]\n",
    "            right_idxs = [362, 263, 385, 380, 387, 373]\n",
    "\n",
    "            if has_sunglasses:\n",
    "                # Pakai Kacamata Hitam -> Labeli Sunglasses\n",
    "                labels.append(f\"{CLS_SUNGLASSES} {get_bbox_str([landmarks[i] for i in left_idxs], h, w)}\")\n",
    "                labels.append(f\"{CLS_SUNGLASSES} {get_bbox_str([landmarks[i] for i in right_idxs], h, w)}\")\n",
    "            else:\n",
    "                # Normal -> Hitung EAR & Cek Side View\n",
    "                l_ear = (calc_dist(landmarks[160], landmarks[144]) + calc_dist(landmarks[158], landmarks[153])) / (2 * calc_dist(landmarks[33], landmarks[133]))\n",
    "                r_ear = (calc_dist(landmarks[385], landmarks[380]) + calc_dist(landmarks[387], landmarks[373])) / (2 * calc_dist(landmarks[362], landmarks[263]))\n",
    "                \n",
    "                l_cls = CLS_C_EYES if l_ear < EAR_THRESH else CLS_O_EYES\n",
    "                r_cls = CLS_C_EYES if r_ear < EAR_THRESH else CLS_O_EYES\n",
    "                \n",
    "                # Hanya labeli mata yang terlihat jelas (tidak terhalang hidung karena miring)\n",
    "                if side_status == 'frontal':\n",
    "                    labels.append(f\"{l_cls} {get_bbox_str([landmarks[i] for i in left_idxs], h, w)}\")\n",
    "                    labels.append(f\"{r_cls} {get_bbox_str([landmarks[i] for i in right_idxs], h, w)}\")\n",
    "                elif side_status == 'look_left': # Kiri dekat, Kanan jauh/hilang\n",
    "                    labels.append(f\"{l_cls} {get_bbox_str([landmarks[i] for i in left_idxs], h, w)}\")\n",
    "                elif side_status == 'look_right': # Kanan dekat, Kiri jauh/hilang\n",
    "                    labels.append(f\"{r_cls} {get_bbox_str([landmarks[i] for i in right_idxs], h, w)}\")\n",
    "\n",
    "            # 6. Label MULUT (Cek Oklusi Tangan & Masker)\n",
    "            mouth_idxs = [61, 291, 39, 181, 0, 17]\n",
    "            \n",
    "            if is_mouth_occluded(frame_enhanced, landmarks, results_hands, h, w):\n",
    "                # Tertutup -> Labeli Covered Mouth\n",
    "                labels.append(f\"{CLS_COVERED_MOUTH} {get_bbox_str([landmarks[i] for i in mouth_idxs], h, w)}\")\n",
    "            else:\n",
    "                # Normal -> Hitung MAR\n",
    "                mouth_h = calc_dist(landmarks[13], landmarks[14])\n",
    "                mouth_w = calc_dist(landmarks[61], landmarks[291])\n",
    "                mar = mouth_h / mouth_w\n",
    "                m_cls = CLS_O_MOUTH if mar > MAR_THRESH else CLS_C_MOUTH\n",
    "                labels.append(f\"{m_cls} {get_bbox_str([landmarks[i] for i in mouth_idxs], h, w)}\")\n",
    "            \n",
    "            # Simpan\n",
    "            save_data(img_out_dir, lbl_out_dir, file_prefix, frame_count, frame_enhanced, labels)\n",
    "            \n",
    "        cap.release()\n",
    "\n",
    "    print(\"PROSES SELESAI: Dataset Final telah dibuat\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca984862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# ==========================================\n",
    "# KONFIGURASI\n",
    "# ==========================================\n",
    "# Arahkan ke folder dataset yang baru saja Anda generate\n",
    "DATASET_DIR = \"data/yolo_dataset_master/images/train\" \n",
    "LABEL_DIR   = \"data/yolo_dataset_master/labels/train\"\n",
    "\n",
    "# Definisi Kelas (Harus urut sesuai prepare_data.py)\n",
    "CLASS_NAMES = [\n",
    "    'Face',           # 0\n",
    "    'Open Eyes',      # 1\n",
    "    'Closed Eyes',    # 2\n",
    "    'Open Mouth',     # 3\n",
    "    'Closed Mouth',   # 4\n",
    "    'Covered Mouth',  # 5 (Tangan/Masker)\n",
    "    'Sunglasses'      # 6 (Kacamata Hitam)\n",
    "]\n",
    "\n",
    "# Warna untuk setiap kelas (B-G-R)\n",
    "COLORS = [\n",
    "    (255, 255, 255), # Face (Putih)\n",
    "    (0, 255, 0),     # Open Eyes (Hijau)\n",
    "    (0, 0, 255),     # Closed Eyes (Merah)\n",
    "    (0, 255, 255),   # Open Mouth (Kuning)\n",
    "    (255, 0, 0),     # Closed Mouth (Biru)\n",
    "    (0, 165, 255),   # Covered (Oranye)\n",
    "    (50, 50, 50)     # Sunglasses (Hitam/Abu)\n",
    "]\n",
    "\n",
    "def view_dataset():\n",
    "    # Ambil semua file gambar\n",
    "    image_paths = sorted(glob.glob(os.path.join(DATASET_DIR, \"*.jpg\")))\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(f\"Tidak ada gambar ditemukan di {DATASET_DIR}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Ditemukan {len(image_paths)} gambar.\")\n",
    "    print(\"NAVIGASI: [D] Next | [A] Previous | [Q] Quit\")\n",
    "\n",
    "    idx = 0\n",
    "    while True:\n",
    "        img_path = image_paths[idx]\n",
    "        \n",
    "        # Cari file label yang sesuai\n",
    "        # Asumsi struktur folder: images/train/x.jpg -> labels/train/x.txt\n",
    "        filename = os.path.basename(img_path).replace(\".jpg\", \".txt\")\n",
    "        lbl_path = os.path.join(LABEL_DIR, filename)\n",
    "\n",
    "        frame = cv2.imread(img_path)\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # Baca Label\n",
    "        if os.path.exists(lbl_path):\n",
    "            with open(lbl_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                cls_id = int(parts[0])\n",
    "                x_c, y_c, bw, bh = map(float, parts[1:])\n",
    "\n",
    "                # Konversi YOLO (Norm) ke Pixel\n",
    "                x1 = int((x_c - bw / 2) * w)\n",
    "                y1 = int((y_c - bh / 2) * h)\n",
    "                x2 = int((x_c + bw / 2) * w)\n",
    "                y2 = int((y_c + bh / 2) * h)\n",
    "\n",
    "                # Gambar Kotak & Teks\n",
    "                color = COLORS[cls_id] if cls_id < len(COLORS) else (255,255,255)\n",
    "                label_text = CLASS_NAMES[cls_id] if cls_id < len(CLASS_NAMES) else str(cls_id)\n",
    "\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(frame, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"NO LABEL FILE\", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "        # Tampilkan Info File\n",
    "        cv2.putText(frame, f\"File: {idx+1}/{len(image_paths)}\", (10, h - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n",
    "        cv2.imshow(\"Dataset Viewer\", frame)\n",
    "\n",
    "        # Input Keyboard\n",
    "        key = cv2.waitKey(0) & 0xFF\n",
    "        if key == ord('d'): # Next\n",
    "            idx = (idx + 1) % len(image_paths)\n",
    "        elif key == ord('a'): # Prev\n",
    "            idx = (idx - 1) % len(image_paths)\n",
    "        elif key == ord('q'): # Quit\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    view_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
